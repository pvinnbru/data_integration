{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def compute_average_ratings(user_ratings_df):\n",
    "    \"\"\"\n",
    "    Compute the average rating for each dive site.\n",
    "    \"\"\"\n",
    "    avg_ratings = user_ratings_df.groupby(\"dive_site_id\")[\"rating\"].mean().reset_index()\n",
    "    avg_ratings.columns = [\"dive_site_id\", \"average_rating\"]\n",
    "    return avg_ratings\n",
    "\n",
    "\n",
    "def build_user_profile(user_id, user_ratings_df, dive_sites_df):\n",
    "    \"\"\"\n",
    "    Builds a user profile indicating the likelihood of the user liking each cluster.\n",
    "    \"\"\"\n",
    "    # Merge ratings with dive sites to access clusters\n",
    "    merged_df = pd.merge(user_ratings_df, dive_sites_df, left_on=\"dive_site_id\", right_on=\"id\")\n",
    "    \n",
    "    # Filter user's ratings\n",
    "    user_data = merged_df[merged_df[\"user_id\"] == user_id]\n",
    "    \n",
    "    # Calculate average rating per cluster\n",
    "    cluster_preferences = user_data.groupby(\"cluster_x\")[\"rating\"].mean().reset_index()\n",
    "    cluster_preferences.columns = [\"cluster\", \"preference_score\"]\n",
    "    \n",
    "    # Normalize preference scores (optional for consistent scaling)\n",
    "    cluster_preferences[\"preference_score\"] /= cluster_preferences[\"preference_score\"].sum()\n",
    "    \n",
    "    return cluster_preferences.sort_values(by=\"preference_score\", ascending=False)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def recommend_from_clusters(user_id, cluster_preferences, user_ratings_df, dive_sites_df, avg_ratings, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommends dive sites based on the user's cluster preferences.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Get dive sites the user has already rated\n",
    "    rated_sites = user_ratings_df[user_ratings_df[\"user_id\"] == user_id][\"dive_site_id\"].values\n",
    "    \n",
    "    # Filter dive sites the user hasn't rated\n",
    "    unrated_sites = dive_sites_df[~dive_sites_df[\"id\"].isin(rated_sites)]\n",
    "    \n",
    "    # Merge unrated sites with average ratings\n",
    "    unrated_sites = pd.merge(unrated_sites, avg_ratings, left_on=\"id\", right_on=\"dive_site_id\", how=\"left\")\n",
    "    \n",
    "    # Calculate number of recommendations per cluster\n",
    "    cluster_preferences[\"num_recommendations\"] = (\n",
    "        (cluster_preferences[\"preference_score\"] * top_n).round().astype(int)\n",
    "    )\n",
    "    \n",
    "    recommendations = []\n",
    "    for _, row in cluster_preferences.iterrows():\n",
    "        cluster = row[\"cluster\"]\n",
    "        num_recommendations = row[\"num_recommendations\"]\n",
    "        \n",
    "        # Get dive sites from the current cluster\n",
    "        cluster_sites = unrated_sites[unrated_sites[\"cluster\"] == cluster]\n",
    "        \n",
    "        # Sort sites by average rating\n",
    "        cluster_sites = cluster_sites.sort_values(by=\"average_rating\", ascending=False)\n",
    "        \n",
    "        # Add the top sites from the cluster to recommendations\n",
    "        recommendations.extend(cluster_sites.head(int(num_recommendations)).to_dict(\"records\"))\n",
    "    \n",
    "    # Shuffle the recommendations to avoid cluster grouping in the result\n",
    "    np.random.shuffle(recommendations)\n",
    "    \n",
    "    # Return the top N recommendations\n",
    "    return recommendations[:top_n]\n",
    "\n",
    "def is_cluster_in_preferred_clusters(cluster, preferred_clusters):\n",
    "    \"\"\"\n",
    "    Checks if a cluster is in the list of preferred clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster (int): The recommended cluster.\n",
    "    preferred_clusters (list): List of preferred clusters.\n",
    "    \n",
    "    Returns:\n",
    "    bool:True if the cluster exists in the preferred clusters, otherwise False.\n",
    "    \"\"\"\n",
    "    return cluster in ast.literal_eval(preferred_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  user_id           preferred_regions preferred_animals  \\\n",
      "1           1        2  ['South Korea', 'Denmark']        [414, 223]   \n",
      "\n",
      "     preferred_types preferred_clusters  \n",
      "1  ['Pool', 'River']             [0, 7]  \n",
      "   Unnamed: 0    id                     title       lat      long  \\\n",
      "0        2611  2606      Loch Low-Minn Quarry  35.48219 -84.50952   \n",
      "1        2319  2313               La Catedral  38.95109   1.52769   \n",
      "2        1961  1961          Kraken Freighter  28.44390 -94.28613   \n",
      "3        1486  1470     El Arco de San Andrés  36.99124  -1.88652   \n",
      "4        3503  3493             PIEDRAS ALTAS  36.71949  -3.73396   \n",
      "5        1108  1083  Grayton Beach State Park  30.30769 -86.15891   \n",
      "6        3085  3074                Hog Heaven  26.13500 -80.07900   \n",
      "7        4299  4297                 3 - ROCKS  36.75074  26.99066   \n",
      "8         138   111  Reserva Marina del Cavet  41.06369   1.07846   \n",
      "9        3024  3015        Sea Hawk Dive Boat  40.63768 -73.58187   \n",
      "\n",
      "                                         description  \\\n",
      "0  Loch Low‑Minn is a ten-acre quarry lake in Ath...   \n",
      "1  Maximum depth: 15 meters Minimum level: Open W...   \n",
      "2  Dubbed the Kraken after the mythical, squid-li...   \n",
      "3  Comenzaremos la inmersión sobre un fondo rocos...   \n",
      "4  Una maravillosa inmersión en caribeña donde po...   \n",
      "5  Grayton Beach State Park - Shore Dive Turtle Reef   \n",
      "6  The main feature of this dive site is a 185' b...   \n",
      "7  Dive into 3-Rocks, located next to Kastri isla...   \n",
      "8  It is an ideal area for shallow dives since it...   \n",
      "9  The Sea Hawk is a great dive boat operating ou...   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "1  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "2  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "3  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "4  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "5  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "6  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "7  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "8  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "9  https://d2p1cf6997m1ir.cloudfront.net/media/th...   \n",
      "\n",
      "                                                 url  max_depth  \\\n",
      "0  https://www.padi.com/dive-site/united-states-o...        NaN   \n",
      "1  https://www.padi.com/dive-site/spain/la-catedr...        NaN   \n",
      "2  https://www.padi.com/dive-site/united-states-o...        NaN   \n",
      "3  https://www.padi.com/dive-site/spain/el-arco-d...        NaN   \n",
      "4  https://www.padi.com/dive-site/spain/piedras-a...        NaN   \n",
      "5  https://www.padi.com/dive-site/united-states-o...        NaN   \n",
      "6  https://www.padi.com/dive-site/united-states-o...        NaN   \n",
      "7   https://www.padi.com/dive-site/greece/3-rocks-3/        NaN   \n",
      "8  https://www.padi.com/dive-site/spain/reserva-m...        NaN   \n",
      "9  https://www.padi.com/dive-site/united-states-o...        NaN   \n",
      "\n",
      "          region  ...                                name dive_site_id_y  \\\n",
      "0  United States  ...                          ['Quarry']         2606.0   \n",
      "1          Spain  ...                            ['Cave']            NaN   \n",
      "2  United States  ...                  ['Wreck', 'Ocean']         1961.0   \n",
      "3          Spain  ...                            ['Wall']         1470.0   \n",
      "4          Spain  ...                            ['Wall']         3493.0   \n",
      "5  United States  ...                   ['Beach', 'Reef']            NaN   \n",
      "6  United States  ...  ['Wreck', 'Sandy bottom', 'Ocean']            NaN   \n",
      "7         Greece  ...                            ['Reef']         4297.0   \n",
      "8          Spain  ...   ['Beach', 'Reef', 'Sandy bottom']            NaN   \n",
      "9  United States  ...                  ['Wreck', 'Ocean']         3015.0   \n",
      "\n",
      "                                      animal_id cluster     pca_1     pca_2  \\\n",
      "0                                          [59]       2 -0.658346 -0.422991   \n",
      "1                                           NaN       2 -0.635363 -0.402920   \n",
      "2      [170, 8, 20, 7, 73, 22, 154, 70, 38, 10]       5  0.707425 -0.598770   \n",
      "3                                [19, 20, 8, 1]       6  0.168007 -0.488837   \n",
      "4               [318, 52, 144, 49, 326, 19, 88]       6 -0.275042 -0.392947   \n",
      "5                                           NaN       0 -0.538159  0.247127   \n",
      "6                                           NaN       5 -0.556508 -0.354171   \n",
      "7                  [178, 57, 170, 494, 20, 361]       0 -0.229727  0.375955   \n",
      "8                                           NaN       0 -0.487426  0.333392   \n",
      "9  [376, 374, 192, 181, 375, 77, 119, 407, 195]       5 -0.470399 -0.501853   \n",
      "\n",
      "      tsne_1     tsne_2  dive_site_id  average_rating  \n",
      "0 -27.243118 -25.264183        2606.0             5.0  \n",
      "1 -33.358086 -11.236391        2313.0             5.0  \n",
      "2  12.672187 -35.861730        1961.0             5.0  \n",
      "3 -33.588570  21.207642        1470.0             5.0  \n",
      "4 -56.517483  26.324387        3493.0             5.0  \n",
      "5  13.271947  -6.951528        1083.0             5.0  \n",
      "6  10.446303 -28.573215        3074.0             5.0  \n",
      "7  35.525130  -2.914584        4297.0             5.0  \n",
      "8   8.803571  -6.266543         111.0             5.0  \n",
      "9 -23.637720 -32.233673        3015.0             5.0  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "user_ratings_df = pd.read_csv(\"../user_ratings_data.csv\")\n",
    "dive_sites_df = pd.read_csv(\"../dive_sites.csv\")  \n",
    "features = pd.read_csv('../preferences.csv')\n",
    "\n",
    "# Compute average ratings\n",
    "avg_ratings = compute_average_ratings(user_ratings_df)\n",
    "\n",
    "i = int(input(\"Enter the user ID to calculate recommendations for: \"))\n",
    "    \n",
    "ft = features[features['user_id']==i]\n",
    "\n",
    "# Build user profile\n",
    "cluster_preferences = build_user_profile(i, user_ratings_df, dive_sites_df)\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = recommend_from_clusters(i, cluster_preferences, user_ratings_df, dive_sites_df, avg_ratings, top_n=10)\n",
    "    \n",
    "print(ft)\n",
    "print(pd.DataFrame(recommendations[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Basierte Ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_average_ratings(user_ratings_df):\n",
    "    \"\"\"\n",
    "    Compute the weighted average rating for each dive site, incorporating the number of reviews.\n",
    "    \"\"\"\n",
    "    # Count the number of reviews for each dive site\n",
    "    review_counts = user_ratings_df.groupby(\"dive_site_id\")[\"rating\"].count().reset_index()\n",
    "    review_counts.columns = [\"dive_site_id\", \"review_count\"]\n",
    "    \n",
    "    # Compute the average rating\n",
    "    avg_ratings = user_ratings_df.groupby(\"dive_site_id\")[\"rating\"].mean().reset_index()\n",
    "    avg_ratings.columns = [\"dive_site_id\", \"average_rating\"]\n",
    "    \n",
    "    # Merge review counts with average ratings\n",
    "    avg_ratings = pd.merge(avg_ratings, review_counts, on=\"dive_site_id\")\n",
    "    \n",
    "    # Normalize review counts to a reasonable range (optional)\n",
    "    avg_ratings[\"review_weight\"] = avg_ratings[\"review_count\"] / avg_ratings[\"review_count\"].max()\n",
    "    \n",
    "    # Apply weight to the average rating\n",
    "    avg_ratings[\"weighted_rating\"] = avg_ratings[\"average_rating\"] * avg_ratings[\"review_weight\"]\n",
    "    \n",
    "    return avg_ratings\n",
    "\n",
    "\n",
    "def build_user_profile(user_id, user_ratings_df, dive_sites_df):\n",
    "    \"\"\"\n",
    "    Builds a user profile indicating the likelihood of the user liking each cluster.\n",
    "    \"\"\"\n",
    "    # Merge ratings with dive sites to access clusters\n",
    "    merged_df = pd.merge(user_ratings_df, dive_sites_df, left_on=\"dive_site_id\", right_on=\"id\")\n",
    "    \n",
    "    # Filter user's ratings\n",
    "    user_data = merged_df[merged_df[\"user_id\"] == user_id]\n",
    "    \n",
    "        # Calculate average rating per cluster\n",
    "    cluster_preferences = user_data.groupby(\"cluster_x\")[\"rating\"].mean().reset_index()\n",
    "    cluster_preferences.columns = [\"cluster\", \"preference_score\"]\n",
    "    \n",
    "    # Normalize preference scores (optional for consistent scaling)\n",
    "    cluster_preferences[\"preference_score\"] /= cluster_preferences[\"preference_score\"].sum()\n",
    "    \n",
    "    return cluster_preferences.sort_values(by=\"preference_score\", ascending=False)\n",
    "\n",
    "\n",
    "def recommend_from_clusters_site(user_id, cluster_preferences, user_ratings_df, dive_sites_df, avg_ratings, top_n=10, top_n_preferences=1):\n",
    "    \"\"\"\n",
    "    Recommends dive sites based on the user's cluster preferences.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Get dive sites the user has already rated\n",
    "    rated_sites = user_ratings_df[user_ratings_df[\"user_id\"] == user_id][\"dive_site_id\"].values\n",
    "    \n",
    "    # Filter dive sites the user hasn't rated\n",
    "    unrated_sites = dive_sites_df[~dive_sites_df[\"id\"].isin(rated_sites)]\n",
    "    \n",
    "    # Merge unrated sites with average ratings (use weighted ratings)\n",
    "    unrated_sites = pd.merge(unrated_sites, avg_ratings[[\"dive_site_id\", \"weighted_rating\"]], \n",
    "                         left_on=\"id\", right_on=\"dive_site_id\", how=\"left\")\n",
    "    \n",
    "    cluster_preferences = cluster_preferences.head(top_n_preferences)\n",
    "    \n",
    "    # Calculate number of recommendations per cluster\n",
    "    cluster_preferences[\"num_recommendations\"] = (\n",
    "        (cluster_preferences[\"preference_score\"] * top_n ).round().astype(int)\n",
    "    )\n",
    "    \n",
    "    recommendations = []\n",
    "    for _, row in cluster_preferences.iterrows():\n",
    "        cluster = row[\"cluster\"]\n",
    "        num_recommendations = row[\"num_recommendations\"]\n",
    "        \n",
    "        # Get dive sites from the current cluster\n",
    "        cluster_sites = unrated_sites[unrated_sites[\"cluster\"] == cluster]\n",
    "        \n",
    "        # Sort sites by average rating\n",
    "        cluster_sites = cluster_sites.sort_values(by=\"weighted_rating\", ascending=False)\n",
    "        \n",
    "        # Add the top sites from the cluster to recommendations\n",
    "        recommendations.extend(cluster_sites.head(int(num_recommendations)).to_dict(\"records\"))\n",
    "    \n",
    "    # Shuffle the recommendations to avoid cluster grouping in the result\n",
    "    np.random.shuffle(recommendations)\n",
    "    \n",
    "    # Return the top N recommendations\n",
    "    return recommendations[:top_n]\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def recommend_regions_with_sites(recommendations, dive_sites_df, top_n_regions=3, max_sites_per_region=3, min_sites_per_region=15):\n",
    "    \"\"\"\n",
    "    Groups recommendations by region, introduces randomness to the region order, \n",
    "    and selects top regions with dive site recommendations, preferring lower indices.\n",
    "    Enforces a minimum number of dive sites per region.\n",
    "    \"\"\"\n",
    "    # Convert recommendations to DataFrame\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "    # Group recommendations by region\n",
    "    region_groups = recommendations_df.groupby(\"region\")\n",
    "    \n",
    "    # Filter regions with at least the minimum number of dive sites\n",
    "    valid_regions = dive_sites_df.groupby(\"region\").filter(lambda x: len(x) >= min_sites_per_region)[\"region\"].unique()\n",
    "    recommendations_df = recommendations_df[recommendations_df[\"region\"].isin(valid_regions)]\n",
    "\n",
    "    # Recalculate region scores (e.g., sum of site ratings in the region)\n",
    "    region_scores = recommendations_df.groupby(\"region\")[\"weighted_rating\"].count().reset_index()\n",
    "    region_scores.columns = [\"region\", \"region_score\"]\n",
    "    \n",
    "    # Sort regions by score, introduce randomness in sorting for ties\n",
    "    region_scores[\"random_tiebreaker\"] = [random.random() for _ in range(len(region_scores))]\n",
    "    top_regions = (\n",
    "        region_scores.sort_values(by=[\"region_score\", \"random_tiebreaker\"], ascending=[False, True])\n",
    "        .head(top_n_regions)\n",
    "    )\n",
    "\n",
    "    # Prepare region recommendations\n",
    "    region_recommendations = []\n",
    "    for region in top_regions[\"region\"]:\n",
    "        # Get top dive sites in the region, prioritize lower indices\n",
    "        region_sites = recommendations_df[recommendations_df[\"region\"] == region] \\\n",
    "                       .sort_values(by=[\"weighted_rating\", \"id\"], ascending=[False, True]) \\\n",
    "                       .head(max_sites_per_region)\n",
    "        region_recommendations.append({\n",
    "            \"region\": region,\n",
    "            \"top_sites\": region_sites[[\"id\", \"title\", \"name\", \"cluster\", \"weighted_rating\"]]\n",
    "        })\n",
    "\n",
    "    # Shuffle the region recommendations for further randomness\n",
    "    region_recommendations\n",
    "    \n",
    "    return region_recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region: Curacao\n",
      "Top Dive Sites:\n",
      "      id                title               name  cluster  weighted_rating\n",
      "16  1938  Blue Bay The Garden           ['Reef']        0           2.9375\n",
      "7   1720        Water Factory  ['Beach', 'Reef']        0           2.8750\n",
      "5   2406           Santa Cruz           ['Reef']        0           2.8125\n",
      "14   762         Rif St Marie           ['Reef']        0           2.7500\n",
      "9    771      Mushroom Forest           ['Reef']        0           2.7500\n",
      "\n",
      "Region: Puerto Rico\n",
      "Top Dive Sites:\n",
      "    id                                          title               name  \\\n",
      "4   88  Escambron Marine Park (La poza del Escambron)  ['Reef', 'Ocean']   \n",
      "10  89                                   Estella Reef  ['Beach', 'Reef']   \n",
      "\n",
      "    cluster  weighted_rating  \n",
      "4         0           2.3125  \n",
      "10        0           2.3125  \n",
      "\n",
      "Region: Egypt\n",
      "Top Dive Sites:\n",
      "      id        title               name  cluster  weighted_rating\n",
      "11  1144     Daedalus           ['Reef']        0           2.6875\n",
      "0    726  El Shugarat  ['Drift', 'Reef']        0           2.5625\n",
      "\n",
      "Region: East Timor\n",
      "Top Dive Sites:\n",
      "      id       title                               name  cluster  \\\n",
      "15  3553  Inner Reef  ['Reef', 'Sandy bottom', 'Ocean']        0   \n",
      "1   3511  Inner Reef  ['Reef', 'Sandy bottom', 'Ocean']        0   \n",
      "\n",
      "    weighted_rating  \n",
      "15           2.5625  \n",
      "1            2.3125  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_21868\\3720663950.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_preferences[\"num_recommendations\"] = (\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "user_ratings_df = pd.read_csv(\"../user_ratings_data.csv\")\n",
    "dive_sites_df = pd.read_csv(\"../dive_sites.csv\")  \n",
    "features = pd.read_csv('../preferences.csv')\n",
    "\n",
    "user_id = 3\n",
    "\n",
    "# Schritt 1: Berechne durchschnittliche Ratings\n",
    "avg_ratings = compute_average_ratings(user_ratings_df)\n",
    "\n",
    "# Schritt 2: Erstelle Benutzerprofil\n",
    "cluster_preferences = build_user_profile(user_id, user_ratings_df, dive_sites_df)\n",
    "\n",
    "# Schritt 3: Empfehle Dive Sites mit gewichteter Sampling\n",
    "recommendations = recommend_from_clusters_site(\n",
    "    user_id, cluster_preferences, user_ratings_df, dive_sites_df, avg_ratings, top_n=100\n",
    ")\n",
    "\n",
    "# Schritt 4: Finde beste Regionen und ihre Dive Sites\n",
    "region_recommendations = recommend_regions_with_sites(\n",
    "    recommendations, dive_sites_df, top_n_regions=5, max_sites_per_region=5, min_sites_per_region=10\n",
    ")\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for rec in region_recommendations:\n",
    "    print(f\"\\nRegion: {rec['region']}\")\n",
    "    print(\"Top Dive Sites:\")\n",
    "    print(rec[\"top_sites\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
